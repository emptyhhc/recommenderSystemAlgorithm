**1、损失函数**  
损失函数是用来度量模型预测错误程度的，在链式法则的梯度下降中无用。常用来对比不同模型在同一数据集下的效果好坏，以及用来判断模型是否收敛，从而停止训练。

**2、MSE损失函数**  
MSE中 (y_true - y_predict)**2, y_true在前，在链式法则中求偏导时无需确保其为正。

**3、经验风险和结构化风险**  
经验风险是指训练集中样本的平均损失，而结构化风险是指模型本身的风险。结构风险最小化等价与正则化，是为了防止过拟合现象。求解最优模型就是最小化经验风险和结构化风险，即$\min \frac{1}{N} \sum_{i=1}^{N} L(y_i, f(x_i)) + \lambda J(f)$，其中加号前面是经验风险、后面是模型复杂度，也就是结构化风险，$\lambda$是系数，用来平衡二者。

**4、正则化**  
正则化是为了抑制过拟合现象，L1、L2正则化分别对应贝叶斯网络的拉普拉斯先验分布和高斯先验分布。  

L1正则化：  

$L(w) = \frac{1}{N}\sum_{i=1}^N(f(x_i;w)^2-y_i)^2+\lambda\lVert w\rVert_1$

L1正则化会减少参数量，让参数尽可能为0，使模型更加简洁

L2正则化:

$L(w) = \frac{1}{N}\sum_{i=1}^N(f(x_i;w)^2-y_i)^2+\frac{\lambda}{2}\lVert w\rVert^2$

L2正则化会使参数值变小，尽可能平缓

**5、精确率和召回率**

分类器分类结果分为4类

* TP：将正类预测为正类  
* FN：将正类预测为负类  
* TN：将负类预测为负类  
* FP：将负类预测为正类

精确率(precision) 为测试集中预测正确的正例比预测为正例的样本数

$P = \frac{TP}{TP+FP}$

召回率(recall)为测试集中预测正确的正例数比真实正例数

$R = \frac{TP}{TP+FN}$

F1值为精确率和召回率的调和均值

$F1 = \frac{1}{\frac{\frac{1}{P}+\frac{1}{R}}{2}} = \frac{2PR}{P+R}$

**6、高斯分布**  
如果某个随机变量的取值范围为实数且我们对其概率分布一无所知的时候，通常会假设它符合正态分布。
这是因为：
* 建模任务的真实分布通常都是符合正态分布的。中心极限定理表明，多个独立随机变量的和近似正态分布。
* 在具有相同方差的所有可能概率分布中，正态分布的熵最大（即不确定性最大）。


**7、一维正态分布**  
正态分布的概率密度函数为
$p(x) = \frac{1}{\sqrt{2\pi}\delta}e^{-\frac{(x-\mu)^2}{2\delta^2}}$，其中$-\infin\leq x\leq \infin$，$\mu$为期望，$\delta$为标准差。

有限个相互独立的正态随机变量的线性组合仍服从正态分布。

若$X_i\sim N(\mu_i, \delta_i^2), i=1,2,\ldots,n$，且他们相互独立，那么它们的线性组合$C_1X_1+C_2X_2+\cdots+C_nX_n$仍然服从正态分布，其中$C_1,C_2,C_3,\ldots,C_n$为不全为0的常数。且

$C_1X_1+C_2X_2+\cdots+C_nX_n\sim N(\sum_{i=0}^nC_i\mu_i, \sum_{i=0}^nC_i^2\mu_i^2)$

**8、Beta分布**

Beta分布是定义在（0，1）之间的连续概率分布。
如果随机变量X服从Beta分布，那么其概率密度函数为：

$\begin{aligned}
    p(X,\alpha,\beta) &= \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}X^{\alpha-1}(1-X)^{\beta-1} \\
    &= \frac{1}{B(\alpha, \beta)}X^{\alpha-1}(1-X)^{\beta-1}, 0\leq X < 1
\end{aligned}$

记作$Beta(\alpha,\beta)$

* 众数为$\frac{\alpha-1}{\alpha + \beta - 2}$
* 期望为$E[x] = \frac{\alpha}{\alpha+\beta}$
* 方差为$Var[x]=\frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$

此处有图，待补充

**9、二项分布**  
假设只有两种结果，成功为$\phi$,不成功为$1-\phi$，那么二项分布描述了独立重复进行$n$次实验，成功$x$次的概率。

* 概率质量函数为
  
  $p(X=x)=C_n^x \phi^x(1-\phi)^{n-x},x\in\{0,1,2,\cdots,n\}$
* 期望为 $E[x]=n\phi$
* 方差为 $Var[x]=n\phi(1-\phi)$

**10、先验分布与后验分布**  
在贝叶斯学派中， 先验分布 + 数据 = 后验分布

如果不使用先验分布，而是只统计数据来计算分布，则只能代表这一个数据集的数据分布，并不能代表全部数据分布。

**11、自信息和熵**  
对于事件$X=x$来说，自信息为$I(x) = -logP(x)$，自信息是单个输出，如果算它的期望，那就会得到**熵**。

$H(x)=-E_{x\sim P(x)}\lbrack logP(x)\rbrack$

熵刻画了使用真实分布P来识别一个样本所用编码长度的期望（即平均编码吗长度）

**12、条件熵**  
对于随机变量X,Y来说，条件熵$H(Y|X)$表示，在给定随机变量X的条件下，随机变量Y的不确定性。

*它被定义为 ：X给定条件下，Y的条件概率分布的熵对X的期望*

$\begin{aligned}
    H(Y|X)&=E_{x\sim P(x)}\lbrack H(Y|X=x)\rbrack \\
    &= -E_{x,y \sim P(x,y)}\lbrack log(Y|X)\rbrack
\end{aligned}$

根据定义可证：

$H(Y,X) = H(X) + H(Y|X)$

即，描述X,Y的信息为，描述X的信息+给定X的条件下描述Y的信息

**13、KL散度**  
KL散度也成为相对熵。对于给定的随机变量X，它的两个概率分布函数$P(x)$和$Q(x)$的区别可以用KL散度来衡量。


$\begin{aligned}
    D_{KL}(P||Q) &= E_{x\sim P(X)}\lbrack log\frac{P(x)}{Q(x)}\rbrack \\ 
    &=E_{x\sim P(X)}\lbrack logP(x)-logQ(x)\rbrack
\end{aligned}$

* KL散度非负，当其为0时，表明1）对于离散型随机变量，P和Q是同一个分布；2）对于连续型随机变量，P和Q几乎处处相等。
* KL散度不满足对称性，即

    $D_{KL}(P||Q) \not = D_{KL}(Q||P)$

    从直观上来看，当$P(x)$的值较大时，只有$Q(x)$的值较小，才能使得$P(X)\frac{logP(x)}{logQ(x)}$较小。

    当当$P(x)$的值较小时，$Q(x)$的值无论是较大还是较小，$P(X)\frac{logP(x)}{logQ(x)}$的值都会较小

    这就是KL散度不满足对称性的原因。

**14、交叉熵**  
$\begin{aligned}
    H(P,Q) &= H(P) + D_{KL}(P||Q) \\ 
    &=-E_{x\sim P(x)}logQ(x)
\end{aligned}$

* 交叉熵刻画了使用错误分布Q来表示真实分布P中样本的平均编码长度
* P,Q的KL散度刻画了错误分布Q表示真实分布P中样本平均编码长度的增量

*缺个《AI算法工程师手册》上的例子*